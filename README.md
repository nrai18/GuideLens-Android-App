# ğŸ“± GuideLensApp: AI-Powered Visual Navigation for Android

**GuideLensApp** is an accessibility-focused on-device navigation system that helps visually impaired users navigate indoor environments independently.  
Built entirely in **Kotlin** with **Jetpack Compose**, it combines real-time **object detection**, **semantic floor segmentation**, and **intelligent pathfinding** to deliver turn-by-turn **audio guidance** through an intuitive AR interface â€” all processed locally on the device.

---

## ğŸŒŸ Why GuideLensApp?

âœ… **100 % On-Device** â€” No internet required, full privacy  
âœ… **Audio-First Design** â€” Text-to-Speech announcements for all navigation events  
âœ… **Production-Ready** â€” Device-adaptive configuration, robust error handling  
âœ… **Optimized Performance** â€” INT8 quantization, NNAPI acceleration, 15â€“20 FPS  
âœ… **Open Source** â€” GPL-3.0 license, fully documented and community-driven  

---

## ğŸš€ Core Features

### ğŸ§  Computer Vision & Navigation

- **Real-Time Object Detection** â€“ YOLO World v2 (INT8) detects 80 object classes at 640Ã—640 resolution with 150â€“250 ms latency.  
  Supports 16 navigable targets: `chair, door, table, bed, couch, toilet, sink, refrigerator, stairs, person, bottle, cup, laptop, phone, keyboard, mouse`.

- **Semantic Floor Segmentation** â€“ Custom-trained **PP-LiteSeg (INT8)** identifies walkable surfaces.  
  **NEW**: Enhanced with **bilinear filtering** to robustly handle multi-colored and textured floors.

- **Intelligent Pathfinding** â€“ **A\*** search on a down-sampled grid + **VFH (Vector Field Histogram)**.  
  **NEW**: Implements **Hysteresis** and **Low-Pass Filtering** to prevent direction jitter and provide smooth, stable guidance.

- **Pure Pursuit Control** â€“ Robotics-grade trajectory tracking with 100 px look-ahead; generates natural commands:  
  *â€œGo straightâ€*, *â€œBear rightâ€*, *â€œVeer leftâ€*.

### ğŸ’Š Medicine Identifier (NEW)
- **AI-Powered Analysis** â€“ Combines on-device **ML Kit Text Recognition** with **Gemini 2.5 Flash API** via local WiFi server.
- **100% Failsafe** â€“ Automatic Google Search fallback for ALL errors (server offline, HTTP 500, quota exceeded, network issues).
- **Text Filtering** â€“ Intelligent keyword extraction focuses on medicine names, dosages, and medical terms.
- **WiFi-Based** â€“ Connects to local Python server on laptop (no USB required).
- **Instant Summaries** â€“ Reads complex medicine labels and provides concise, spoken summaries (e.g., *"Paracetamol, used for pain relief"*).
- **Privacy-First** â€“ Images are processed securely, with text-only data sent to the API.

---

## â™¿ Accessibility Features

- **Text-to-Speech Integration**
  - â€œNavigating to [object]â€ on start  
  - â€œ[Object] foundâ€ on first detection  
  - Natural, slower speech rate (0.7x) for clarity
  - Turn commands every 2.5 s (max)  
  - â€œArrived at destinationâ€ on goal  
  - â€œNavigation stoppedâ€ on exit

- **Voice Command Control** (Fully Implemented)
  - **"Navigate to [object]"** â€“ Starts navigation hands-free.
  - **"Stop"** â€“ Ends current session.
  - **"Describe Scene"** â€“ Provides a summary of visible objects.
  - **Hold-to-Speak** â€“ Intuitive long-press gesture on the bottom bar.

- **App Polish & Battery Saver**
  - **Smart Lifecycle** â€“ Pauses heavy ML/Camera tasks when app is backgrounded to save battery.
  - **Robust Permissions** â€“ Smart handling of denied permissions with direct settings access.

- **Neon High-Contrast UI**
  - Vibrating colors (Neon Green/Yellow on Black) for maximum visibility.
  - Pulsing animations for active states (Scanning, Target Acquired).

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Details |
|------------|-------------|----------|
| **Platform** | Android API 29+ | Android 10 and later |
| **Language** | Kotlin 100 % | Modern coroutines-based |
| **UI** | Jetpack Compose | Material Design 3 UI |
| **Architecture** | MVVM | `ViewModel`, `StateFlow` separation |
| **ML Runtime** | ONNX Runtime 1.16.0 | Cross-platform INT8 optimized |
| **OCR** | ML Kit Text Recognition | On-device fast extraction |
| **AI API** | Google Gemini 2.5 Flash | Local server + automatic Google fallback |
| **Sensors** | Fusion (Accel/Mag/Gyro) | Stable heading calculation |

**Model Pipeline**

- **YOLO World v2** â†’ PyTorch â†’ ONNX â†’ INT8 Quantization  
- **PP-LiteSeg** â†’ Custom PyTorch Training â†’ ONNX â†’ INT8 Quantization  

**Why ONNX Runtime?**  
15â€“20 % faster INT8 inference than TFLite, superior NNAPI integration, cross-platform portability.

---

## ğŸ§­ Algorithms & Implementation

### ğŸ—ºï¸ A\* Pathfinding & VFH
- Manhattan-distance heuristic.
- **VFH (Vector Field Histogram)** for local obstacle avoidance.
- **Smoothing**: Low-pass filter on output angle ($\alpha = 0.3$).
- **Hysteresis**: Cost bonus to previous sector to prevent decision flipping.

### ğŸ”„ Pure Pursuit Controller
- Curvature $\kappa = 2 \cdot \sin(\alpha) / L$
- Generates natural language commands ("Bear left", "Turn sharp right").

---

## âš™ï¸ Device-Adaptive Configuration

| Tier | FPS | Resolution | ML Threads | Acceleration |
|------|-----|-------------|-------------|---------------|
| **High-End** (â‰¥ 8 GB RAM, â‰¥ 8 cores) | 20 | 1280Ã—720 | 4 | NNAPI + FP16 |
| **Mid-Range** (4â€“6 GB RAM) | 15 | 960Ã—540 | 2 | CPU only INT8 |

Dynamic profiling adjusts thresholds, frame rates, and resolution at runtime.

---

## ğŸ“¦ Installation

### Prerequisites
- Android Studio Hedgehog (2023.1.1+)  
- Android SDK API 24+  
- Physical device with camera (â‰¥ 4 GB RAM recommended 8 GB)
- **Gemini API Key**: Required for Medicine ID feature.

### Setup
```bash
git clone https://github.com/nrai18/GuideLens-Android-App.git
cd "GuideLens App"
```

### Python Server Setup  (Medicine ID â€“ WiFi Connection)
The Medicine ID feature requires a local Python server running on your laptop.

1.  **Install Python Dependencies**:
    ```bash
    pip install flask flask-cors google-generativeai python-dotenv
    ```

2.  **Environment Setup**:
    Create a `.env` file in the root directory:
    ```env
    GEMINI_API_KEY=your_actual_api_key_here
    ```

3.  **Find Your WiFi IP** (Windows):
    ```bash
    ipconfig
    # Look for "IPv4 Address" under WiFi adapter
    ```

4.  **Update App Config**:
    Edit `app/src/main/java/com/example/guidelensapp/Config.kt`:
    ```kotlin
    const val SERVER_IP = "your.laptop.ip.address"  // e.g., "192.168.1.100"
    ```

5.  **Run Server**:
    ```bash
    python server.py
    # Server will run on http://your-ip:5000
    ```

6.  **Connect Phone**:
    - Ensure phone and laptop are on the **same WiFi network**
    - Check server health: `http://your-ip:5000/health` in phone browser
    - If can't connect, check firewall settings

**Automatic Failsafe**: If server is offline or encounters any error, app automatically opens Google Search with filtered keywords.

### Add ML models to `app/src/main/assets/`
 - `yolov8s-worldv2_int8.onnx` (~10 MB)
 - `floor_segmentation_int8.onnx` (~3 MB)

### Build & Run
1.  **File â†’ Sync Project with Gradle**
2.  **Build â†’ Make Project**
3.  **Run â†’ Run 'app'** (grant camera & microphone permissions)

---

## â–¶ï¸ Usage

### Basic Navigation
1. Tap âš™ï¸ to select target object (e.g. *Chair*) or say **"Navigate to Chair"**.
2. Hear â€œNavigating to chairâ€.
3. Follow audio commands: â€œBear rightâ€, â€œMove forwardâ€.
4. Arrival â†’ â€œArrived at destinationâ€.

### Medicine ID
1. Select "Medicine Identifier" from Start Screen.
2. Point camera at medicine box.
3. Say "scan" or tap scan button.
4. Say "search" or "yes" when prompted.
5. Listen to AI summary from Gemini OR Google Search opens automatically if server is offline.

---

## âš™ï¸ Performance & Optimization

### Benchmarks (Nothing Phone 3a, Snapdragon 7s Gen 3)
| Component | Latency | Notes |
|------------|----------|-------|
| Object Detection | 150â€“250 ms | YOLO World INT8 + NNAPI |
| Floor Segmentation | 80â€“120 ms | PP-LiteSeg INT8 |
| Medicine Analysis | 1â€“2 sec | OCR (On-device) + API |
| End-to-End Nav | 2.5â€“4 FPS | Full pipeline |

Memory â‰ˆ **250 MB (active)** Â· Battery â‰ˆ **15â€“20 % per hour**

### Key Optimizations
- **INT8 Quantization** â†’ 4Ã— smaller models, 2â€“3Ã— faster inference  
- **NNAPI Acceleration** â†’ 2Ã— speedup on Snapdragon NPUs  
- **Lifecycle Management** â†’ Auto-pause when backgrounded
- **Memory Pooling** â†’ Bitmap reuse + explicit GC  

---

## ğŸ—ï¸ Project Architecture
```
app/src/main/java/com/example/guidelensapp/
â”œâ”€â”€ MainActivity.kt 
â”œâ”€â”€ GuideLensApplication.kt 
â”œâ”€â”€ Config.kt 
â”œâ”€â”€ viewmodel/
â”‚   â”œâ”€â”€ NavigationViewModel.kt 
â”‚   â””â”€â”€ NavigationUiState.kt
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ ObjectDetector.kt 
â”‚   â”œâ”€â”€ ONNXFloorSegmenter.kt 
â”‚   â””â”€â”€ TextRecognitionManager.kt
â”œâ”€â”€ network/
â”‚   â””â”€â”€ GeminiService.kt
â”œâ”€â”€ sensors/
â”‚   â””â”€â”€ SpatialTracker.kt 
â”œâ”€â”€ navigation/
â”‚   â””â”€â”€ PathPlanner.kt 
â”œâ”€â”€ accessibility/
â”‚   â”œâ”€â”€ TextToSpeechManager.kt 
â”‚   â””â”€â”€ VoiceCommandManager.kt
â””â”€â”€ ui/composables/
    â”œâ”€â”€ SimpleNavigationComposables.kt
    â”œâ”€â”€ StartScreen.kt
    â”œâ”€â”€ MedicineIdScreen.kt
    â””â”€â”€ ...
```

**Data Flow:**  
Camera Frame â†’ ViewModel â†’ [SensorTracker + ObjectDetector â†’ FloorSegmenter â†’ PathPlanner state] â†’ TTS â†’ StateFlow â†’ Compose UI

---

## ğŸ¯ Future Enhancements

- **Dynamic Obstacle Avoidance** with real-time re-planning (Partially Implemented via VFH)
- **Haptic Turn Cues** for tactile feedback  
- **Multi-Object Waypoints & Navigation History**  
- **Sensor Fusion** for improved heading stability (Implemented) 
- **Model Distillation** for total model size under 5 MB  

---

## ğŸ’¡ Accessibility & Impact

**GuideLensApp** empowers users with visual impairments to navigate independently using only a smartphone camera â€” no beacons, maps, or internet required.  
It demonstrates that **real-time, privacy-preserving AI** can be practical on-device, enhancing inclusion and mobility for millions worldwide.

---

## ğŸ“š References & Resources

- [ONNX Runtime Mobile Docs](https://onnxruntime.ai/docs/get-started/with-mobile.html)  
- [Ultralytics YOLO World Docs](https://docs.ultralytics.com/hub/app/android/)  
- [PP-LiteSeg Paper (ArXiv)](https://arxiv.org/html/2504.20976v1)  
- [A* Algorithm â€“ Wikipedia](https://en.wikipedia.org/wiki/A*_search_algorithm)  
- [Pure Pursuit Controller â€“ MathWorks](https://www.mathworks.com/help/robotics/ref/purepursuit.html)  

---

## ğŸ§‘â€ğŸ’» Author & Repository

**Developer:** Naman Rai  
**Repository:** [github.com/nrai18/GuideLens-Android-App](https://github.com/nrai18/GuideLens-Android-App)  
**License:** [GPL-3.0](https://www.gnu.org/licenses/gpl-3.0.html)  

---

## ğŸ Conclusion

**GuideLensApp** showcases the future of **on-device AI navigation** â€” merging deep learning, classical algorithms, and accessibility design into one cohesive Android application.  
With optimized models, adaptive runtime, and robust engineering, it stands as a **reference implementation for real-time computer-vision navigation** on mobile devices.

